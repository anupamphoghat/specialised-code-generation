{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Fine-Tuning Gemma with PEFT/LoRA\n",
    "\n",
    "This notebook performs the core fine-tuning process. We will:\n",
    "1. Load the base `gemma-2b-it` model in 4-bit precision.\n",
    "2. Load our custom dataset.\n",
    "3. Configure LoRA to create trainable adapters.\n",
    "4. Run the training using the `SFTTrainer` from the TRL library.\n",
    "5. Save the resulting adapters for later use.\n",
    "\n",
    "**Note:** This requires a GPU with sufficient VRAM (e.g., NVIDIA T4, V100, or A100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install and Import Dependencies\n",
    "\n",
    "You would first need to install the required libraries. A `requirements.txt` would look like this:\n",
    "```\n",
    "torch\n",
    "transformers\n",
    "bitsandbytes\n",
    "peft\n",
    "trl\n",
    "datasets\n",
    "accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Model and Tokenizer\n",
    "\n",
    "We load the model in 4-bit using `BitsAndBytesConfig` to make it fit into memory. We also prepare it for k-bit training, which stabilizes the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model.config.use_cache = False # Recommended for fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure LoRA\n",
    "\n",
    "Here, we define the LoRA configuration. We specify which layers of the model we want to attach the trainable adapters to. For Gemma, targeting all linear layers (`q_proj`, `k_proj`, `v_proj`, `o_proj`, etc.) is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the update matrices. Lower rank means fewer trainable parameters.\n",
    "    lora_alpha=32, # Alpha scaling factor.\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # Target all linear layers\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters() # This will show how few parameters we are actually training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load Dataset and Set Up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='dataset.jsonl', split='train')\n",
    "\n",
    "# The SFTTrainer needs a function to format the dataset entries\n",
    "def formatting_func(example):\n",
    "    # Note: The exact format depends on the base model's training.\n",
    "    # For Gemma-IT, a specific chat template is expected.\n",
    "    text = f\"<start_of_turn>user\\n{example['prompt']}<end_of_turn>\\n<start_of_turn>model\\n{example['response']}<end_of_turn>\"\n",
    "    return [text]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-pandas-expert\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3, # Use more epochs for a real dataset\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True, # Use fp16 for faster training\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024, # Adjust based on your VRAM\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the LoRA adapters\n",
    "trainer.save_model(\"./gemma-pandas-expert-adapters\")\n",
    "print(\"Model adapters saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
